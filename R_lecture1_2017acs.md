R Basics for Lecture 1
================

## Econ B2000, Statistics and Introduction to Econometrics

## Kevin R Foster, Colin Powell School, the City College of New York, CUNY

For this class we’ll be using the statistical analysis program, R. The
computer labs here on campus have the necessary software. If you have
your own computer, download and install the program from [R
Project](http://www.r-project.org/) along with
[R-Studio](http://www.rstudio.com/). Depending on your particular device
these details will be different so take it patiently and figure it out -
there are sufficient online sources for help, to address any problem
that arises.

The best way to learn it is to just do it. How many times have you got a
new game, skipped reading the instructions, and learned by crashing it a
few times? This isn’t quite so simple but the basic gist remains - try
it. Below I give you some pointers about how to just do it, but they’re
meant to be read then immediately done in real life, I’ll give you some
commands to just copy-and-paste into the program.

The program, R, is the machine underneath that does the work, while
R-Studio is a skin on top that makes it easier to use. Install both then
run R-Studio, and you’ll get something that looks like this: ![image of
R Studio](screenshot_RStudio1.png)

The screen has 4 parts: the Console at lower left (where I drew the
green arrow) is most important since that’s where you type in the
commands to R.

You can start by just copying and pasting commands from this document
into the “Console” and seeing the output from the program.

The guide, *An Introduction to R*, suggests these commands to give a
basic flavor of what’s done and hint at the power. Don’t worry too much
about each step for now, this is just a glimpse to show that with a few
commands you can do some relatively sophisticated estimation – i.e. for
a small cost you can get a large benefit. Copy them and paste into the
“Console”.

``` r
x <- 1:50
w <- 1 + sqrt(x)/2
example1 <- data.frame(x=x, y= x + rnorm(x)*w)
attach(example1)
```

This creates x and y variables (where the rnorm command creates random
numbers from a normal distribution), puts them into a data frame, then
attaches that data frame so that R can use it in later calculations.
(What’s a data frame? A way to collect together related data items.)
Next some stats - create a linear model (that’s the “lm”) then a
“lowess” nonparametric local regression, and plot the two
estimations to compare. (Just copy and paste these, don’t worry about
understanding them for now\!)

``` r
fm <- lm(y ~ x)
summary(fm)
```

    ## 
    ## Call:
    ## lm(formula = y ~ x)
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -5.3922 -2.2656 -0.3014  1.4443  9.4005 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  0.15504    0.94555   0.164     0.87    
    ## x            1.03273    0.03227  32.001   <2e-16 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 3.293 on 48 degrees of freedom
    ## Multiple R-squared:  0.9552, Adjusted R-squared:  0.9543 
    ## F-statistic:  1024 on 1 and 48 DF,  p-value: < 2.2e-16

``` r
lrf <- lowess(x, y)
plot(x, y)
lines(x, lrf$y)
abline(0, 1, lty=3)
abline(coef(fm))
```

![](R_lecture1_2017acs_files/figure-gfm/unnamed-chunk-2-1.png)<!-- -->

``` r
detach()
```

Your own graph should look similar although with the random numbers, not
exactly.

The final “detach” command just cleans up, it is the opposite of
“attach”.

Later you will find that while it is possible to go back and fix up code
that you previously ran in R (the up arrow brings most recent commands),
it is a bit of a pain. It is easier to write out the code in the
upper-left panel, then R-Studio can obediently run all or part of it in
R for you. Then if you make a mistake or just want to do it again, it’s
a bit easier. You can save your code for next time, as well.

One big thing to learn is that while pushing buttons is easier at first,
eventually you want to be able to write code. I won’t insist that you do
that from week \#1, but keep it in mind as a goal to work towards. In R,
you can usually see the code generated by the button-pushes so you can
learn.

For all of these commands, you can use the help box on R Studio. But as
I said, don’t worry much about those commands for now, I’m not expecting
you to become an R-ninja overnight.

# With Some Data

We will go through some basic stats with R using the Census Bureau’s
PUMS (Public Use Microdata Sample, from the American Community Survey,
accessed from IPUMS). I will help you get some quick wins before
circling back to get into more detail.

I have restricted the PUMS data to contain only people living in the
state of New York, a sample of 196,585. The full sample, with 3,190,040
observations, is also on the class website if you’d like – although you
might find that it slows your computer quite a bit\!

You have to learn a bit about your computer’s filing system. When you
download stuff it probably goes into a Download folder. When you
installed R and R-Studio, those went into their own folders (probably
within some Application folder) and then the program might have created
a R folder for your work.

It’s now time to be intentional and organized. Create a folder for your
R work. Within that folder, create a separate folder for each project
that you work on – so for this project, create a folder called
“ecob2000\_lecture1”.

Go and download the PUMS data from the [class
page](http://kfoster.ccny.cuny.edu/classes/fall2020/), which will likely
put that zip file into your Downloads folder. Within that zip file is
one particular file, acs2017\_ny\_data.RData - move that into your new
file, ecob2000\_lecture1.

Note that you can’t just download the zip file and run the program, you
have to extract the particular .RData file – that regularly trips up
people as they begin.

R will look for files in a particular directory. Type the command,
“getwd()” to see where it’s currently looking. Then use the command,
“setwd,” to tell it where it ought to be looking (the
R/ecob2000\_lecture1 folder that you created just now, or wherever you
put that data file). Alt you can click “Session” then “Set Working
Directory” then “Choose Directory” and click the folder, and that will
insert the “setwd” command onto the Console line. Tell R to look for the
data in the folder where you put the data.

Then run these commands (output from those commands is below),

``` r
load("acs2017_ny_data.RData")
#glimpse(acs2017_ny) try this later
acs2017_ny[1:10,1:7]
```

    ##    AGE female educ_nohs educ_hs educ_somecoll educ_college educ_advdeg
    ## 1   72      1         0       0             0            0           1
    ## 2   72      0         0       0             0            0           1
    ## 3   31      0         0       0             0            1           0
    ## 4   28      1         0       0             0            1           0
    ## 5   54      0         0       0             0            0           1
    ## 6   45      1         0       1             0            0           0
    ## 7   84      1         0       0             1            0           0
    ## 8   71      0         0       0             0            1           0
    ## 9   68      1         0       0             1            0           0
    ## 10  37      1         1       0             0            0           0

``` r
attach(acs2017_ny)
```

In the next section I’ll explain more about the data and what the lines
mean but for now AGE is the person’s age in years and female is a 0/1
variable (takes value 1 for true and 0 for false). So if you look at the
output, see that the first person on line 1 is a 72-year-old female,
next is a 72-year-old male, then a 31-year-old male, etc. The set of
variables beginning “educ\_” are also 0/1 so the first 2 people have
advanced degrees and next 2 have college degrees.

You can also use the command, summary, to find out about data.

``` r
summary(acs2017_ny)
```

    ##       AGE            female         educ_nohs        educ_hs      
    ##  Min.   : 0.00   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  
    ##  1st Qu.:22.00   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  
    ##  Median :42.00   Median :1.0000   Median :0.000   Median :0.0000  
    ##  Mean   :41.57   Mean   :0.5156   Mean   :0.271   Mean   :0.2804  
    ##  3rd Qu.:60.00   3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000  
    ##  Max.   :95.00   Max.   :1.0000   Max.   :1.000   Max.   :1.0000  
    ##                                                                   
    ##  educ_somecoll    educ_college     educ_advdeg                  SCHOOL      
    ##  Min.   :0.000   Min.   :0.0000   Min.   :0.000   N/A              :  5569  
    ##  1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.000   No, not in school:144968  
    ##  Median :0.000   Median :0.0000   Median :0.000   Yes, in school   : 46048  
    ##  Mean   :0.173   Mean   :0.1567   Mean   :0.119   Missing          :     0  
    ##  3rd Qu.:0.000   3rd Qu.:0.0000   3rd Qu.:0.000                             
    ##  Max.   :1.000   Max.   :1.0000   Max.   :1.000                             
    ##                                                                             
    ##                         EDUC      
    ##  Grade 12                 :55119  
    ##  4 years of college       :30802  
    ##  5+ years of college      :23385  
    ##  1 year of college        :19947  
    ##  Nursery school to grade 4:14240  
    ##  2 years of college       :14065  
    ##  (Other)                  :39027  
    ##                                           EDUCD      
    ##  Regular high school diploma                 :35689  
    ##  Bachelor's degree                           :30802  
    ##  1 or more years of college credit, no degree:19947  
    ##  Master's degree                             :17010  
    ##  Associate's degree, type not specified      :14065  
    ##  Some college, but less than 1 year          : 9086  
    ##  (Other)                                     :69986  
    ##                                      DEGFIELD     
    ##  N/A                                     :142398  
    ##  Business                                :  9802  
    ##  Education Administration and Teaching   :  6708  
    ##  Social Sciences                         :  4836  
    ##  Medical and Health Sciences and Services:  3919  
    ##  Fine Arts                               :  3491  
    ##  (Other)                                 : 25431  
    ##                                   DEGFIELDD     
    ##  N/A                                   :142398  
    ##  Psychology                            :  2926  
    ##  Business Management and Administration:  2501  
    ##  Accounting                            :  2284  
    ##  General Education                     :  2238  
    ##  English Language and Literature       :  2202  
    ##  (Other)                               : 42036  
    ##                                  DEGFIELD2     
    ##  N/A                                  :190425  
    ##  Business                             :   972  
    ##  Social Sciences                      :   853  
    ##  Education Administration and Teaching:   611  
    ##  Fine Arts                            :   465  
    ##  Communications                       :   352  
    ##  (Other)                              :  2907  
    ##                                                            DEGFIELD2D    
    ##  N/A                                                            :190425  
    ##  Psychology                                                     :   284  
    ##  Economics                                                      :   260  
    ##  Political Science and Government                               :   243  
    ##  Business Management and Administration                         :   217  
    ##  French, German, Latin and Other Common Foreign Language Studies:   205  
    ##  (Other)                                                        :  4951  
    ##       PUMA            GQ           OWNERSHP       OWNERSHPD        MORTGAGE    
    ##  Min.   : 100   Min.   :1.000   Min.   :0.000   Min.   : 0.00   Min.   :0.000  
    ##  1st Qu.:1500   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:12.00   1st Qu.:0.000  
    ##  Median :3201   Median :1.000   Median :1.000   Median :13.00   Median :1.000  
    ##  Mean   :2713   Mean   :1.148   Mean   :1.266   Mean   :14.95   Mean   :1.453  
    ##  3rd Qu.:3902   3rd Qu.:1.000   3rd Qu.:2.000   3rd Qu.:22.00   3rd Qu.:3.000  
    ##  Max.   :4114   Max.   :5.000   Max.   :2.000   Max.   :22.00   Max.   :4.000  
    ##                                                                                
    ##     OWNCOST           RENT         COSTELEC       COSTGAS        COSTWATR   
    ##  Min.   :    0   Min.   :   0   Min.   :   0   Min.   :   0   Min.   :   0  
    ##  1st Qu.: 1208   1st Qu.:   0   1st Qu.: 960   1st Qu.: 840   1st Qu.: 320  
    ##  Median : 2891   Median :   0   Median :1560   Median :2400   Median :1400  
    ##  Mean   :38582   Mean   : 393   Mean   :2311   Mean   :5032   Mean   :4836  
    ##  3rd Qu.:99999   3rd Qu.: 630   3rd Qu.:2520   3rd Qu.:9993   3rd Qu.:9993  
    ##  Max.   :99999   Max.   :3800   Max.   :9997   Max.   :9997   Max.   :9997  
    ##                                                                             
    ##     COSTFUEL       HHINCOME          FOODSTMP        LINGISOL    
    ##  Min.   :   0   Min.   : -11800   Min.   :1.000   Min.   :0.000  
    ##  1st Qu.:9993   1st Qu.:  41600   1st Qu.:1.000   1st Qu.:1.000  
    ##  Median :9993   Median :  81700   Median :1.000   Median :1.000  
    ##  Mean   :7935   Mean   : 114902   Mean   :1.147   Mean   :1.002  
    ##  3rd Qu.:9993   3rd Qu.: 140900   3rd Qu.:1.000   3rd Qu.:1.000  
    ##  Max.   :9997   Max.   :2030000   Max.   :2.000   Max.   :2.000  
    ##                 NA's   :10630                                    
    ##      ROOMS           BUILTYR2         UNITSSTR        FUELHEAT    
    ##  Min.   : 0.000   Min.   : 0.000   Min.   : 0.00   Min.   :0.000  
    ##  1st Qu.: 4.000   1st Qu.: 1.000   1st Qu.: 3.00   1st Qu.:2.000  
    ##  Median : 6.000   Median : 3.000   Median : 3.00   Median :2.000  
    ##  Mean   : 5.887   Mean   : 3.711   Mean   : 4.39   Mean   :2.959  
    ##  3rd Qu.: 8.000   3rd Qu.: 5.000   3rd Qu.: 6.00   3rd Qu.:4.000  
    ##  Max.   :16.000   Max.   :22.000   Max.   :10.00   Max.   :9.000  
    ##                                                                   
    ##       SSMC            FAMSIZE           NCHILD           NCHLT5       
    ##  Min.   :0.00000   Min.   : 1.000   Min.   :0.0000   Min.   :0.00000  
    ##  1st Qu.:0.00000   1st Qu.: 2.000   1st Qu.:0.0000   1st Qu.:0.00000  
    ##  Median :0.00000   Median : 3.000   Median :0.0000   Median :0.00000  
    ##  Mean   :0.01102   Mean   : 3.087   Mean   :0.5009   Mean   :0.08441  
    ##  3rd Qu.:0.00000   3rd Qu.: 4.000   3rd Qu.:1.0000   3rd Qu.:0.00000  
    ##  Max.   :2.00000   Max.   :19.000   Max.   :9.0000   Max.   :5.00000  
    ##                                                                       
    ##      RELATE          RELATED           MARST            RACE          RACED    
    ##  Min.   : 1.000   Min.   : 101.0   Min.   :1.000   Min.   :1.00   Min.   :100  
    ##  1st Qu.: 1.000   1st Qu.: 101.0   1st Qu.:1.000   1st Qu.:1.00   1st Qu.:100  
    ##  Median : 2.000   Median : 201.0   Median :5.000   Median :1.00   Median :100  
    ##  Mean   : 3.307   Mean   : 335.6   Mean   :3.742   Mean   :2.03   Mean   :205  
    ##  3rd Qu.: 3.000   3rd Qu.: 301.0   3rd Qu.:6.000   3rd Qu.:2.00   3rd Qu.:200  
    ##  Max.   :13.000   Max.   :1301.0   Max.   :6.000   Max.   :9.00   Max.   :990  
    ##                                                                                
    ##      HISPAN          HISPAND                  BPL        
    ##  Min.   :0.0000   Min.   :  0.00   New York     :128517  
    ##  1st Qu.:0.0000   1st Qu.:  0.00   West Indies  :  8481  
    ##  Median :0.0000   Median :  0.00   China        :  4964  
    ##  Mean   :0.4153   Mean   : 44.75   SOUTH AMERICA:  4957  
    ##  3rd Qu.:0.0000   3rd Qu.:  0.00   India        :  3476  
    ##  Max.   :4.0000   Max.   :498.00   Pennsylvania :  3303  
    ##                                    (Other)      : 42887  
    ##                  BPLD                            ANCESTR1    
    ##  New York          :128517   Not Reported            :32021  
    ##  China             :  4116   Italian                 :20577  
    ##  Dominican Republic:  3517   Irish, various subheads,:16388  
    ##  Pennsylvania      :  3303   German                  :12781  
    ##  New Jersey        :  3127   African-American        : 9559  
    ##  Puerto Rico       :  2272   United States           : 8209  
    ##  (Other)           : 51733   (Other)                 :97050  
    ##                                    ANCESTR1D             ANCESTR2     
    ##  Not Reported                           :32021   Not Reported:141487  
    ##  Italian (1990-2000, ACS, PRCS)         :20577   German      :  9476  
    ##  Irish                                  :15651   Irish       :  9238  
    ##  German (1990-2000, ACS/PRCS)           :12605   English     :  4895  
    ##  African-American (1990-2000, ACS, PRCS): 9559   Italian     :  4531  
    ##  United States                          : 8209   Polish      :  3113  
    ##  (Other)                                :97963   (Other)     : 23845  
    ##                           ANCESTR2D         CITIZEN          YRSUSA1      
    ##  Not Reported                  :141487   Min.   :0.0000   Min.   : 0.000  
    ##  German (1990-2000, ACS, PRCS) :  9441   1st Qu.:0.0000   1st Qu.: 0.000  
    ##  Irish                         :  8809   Median :0.0000   Median : 0.000  
    ##  English                       :  4895   Mean   :0.4793   Mean   : 5.377  
    ##  Italian (1990-2000, ACS, PRCS):  4531   3rd Qu.:0.0000   3rd Qu.: 0.000  
    ##  Polish                        :  3113   Max.   :3.0000   Max.   :92.000  
    ##  (Other)                       : 24309                                    
    ##     HCOVANY         HCOVPRIV         SEX            EMPSTAT     
    ##  Min.   :1.000   Min.   :1.000   Male  : 95222   Min.   :0.000  
    ##  1st Qu.:2.000   1st Qu.:1.000   Female:101363   1st Qu.:1.000  
    ##  Median :2.000   Median :2.000                   Median :1.000  
    ##  Mean   :1.951   Mean   :1.691                   Mean   :1.514  
    ##  3rd Qu.:2.000   3rd Qu.:2.000                   3rd Qu.:3.000  
    ##  Max.   :2.000   Max.   :2.000                   Max.   :3.000  
    ##                                                                 
    ##     EMPSTATD        LABFORCE          OCC              IND       
    ##  Min.   : 0.00   Min.   :0.000   0      : 79987   0      :79987  
    ##  1st Qu.:10.00   1st Qu.:1.000   2310   :  3494   7860   : 9025  
    ##  Median :10.00   Median :2.000   5700   :  3235   8680   : 6354  
    ##  Mean   :15.16   Mean   :1.331   430    :  3025   770    : 6279  
    ##  3rd Qu.:30.00   3rd Qu.:2.000   4720   :  2666   8190   : 5873  
    ##  Max.   :30.00   Max.   :2.000   4760   :  2563   7870   : 4041  
    ##                                  (Other):101615   (Other):85026  
    ##     CLASSWKR       CLASSWKRD        WKSWORK2        UHRSWORK    
    ##  Min.   :0.000   Min.   : 0.00   Min.   :0.000   Min.   : 0.00  
    ##  1st Qu.:0.000   1st Qu.: 0.00   1st Qu.:0.000   1st Qu.: 0.00  
    ##  Median :2.000   Median :22.00   Median :1.000   Median :12.00  
    ##  Mean   :1.116   Mean   :13.03   Mean   :2.701   Mean   :19.77  
    ##  3rd Qu.:2.000   3rd Qu.:22.00   3rd Qu.:6.000   3rd Qu.:40.00  
    ##  Max.   :2.000   Max.   :29.00   Max.   :6.000   Max.   :99.00  
    ##                                                                 
    ##      INCTOT           FTOTINC           INCWAGE          POVERTY     
    ##  Min.   :  -7300   Min.   : -11800   Min.   :     0   Min.   :  0.0  
    ##  1st Qu.:   8000   1st Qu.:  35550   1st Qu.:     0   1st Qu.:159.0  
    ##  Median :  25000   Median :  74000   Median : 10000   Median :351.0  
    ##  Mean   :  45245   Mean   : 107111   Mean   : 33796   Mean   :318.7  
    ##  3rd Qu.:  56500   3rd Qu.: 132438   3rd Qu.: 47000   3rd Qu.:501.0  
    ##  Max.   :1563000   Max.   :2030000   Max.   :638000   Max.   :501.0  
    ##  NA's   :31129     NA's   :10817     NA's   :33427                   
    ##     MIGRATE1       MIGRATE1D        MIGPLAC1         MIGCOUNTY1     
    ##  Min.   :0.000   Min.   : 0.00   Min.   :  0.000   Min.   :  0.000  
    ##  1st Qu.:1.000   1st Qu.:10.00   1st Qu.:  0.000   1st Qu.:  0.000  
    ##  Median :1.000   Median :10.00   Median :  0.000   Median :  0.000  
    ##  Mean   :1.122   Mean   :11.51   Mean   :  6.184   Mean   :  4.117  
    ##  3rd Qu.:1.000   3rd Qu.:10.00   3rd Qu.:  0.000   3rd Qu.:  0.000  
    ##  Max.   :4.000   Max.   :40.00   Max.   :900.000   Max.   :810.000  
    ##                                                                     
    ##     MIGPUMA1        VETSTAT          VETSTATD         PWPUMA00    
    ##  Min.   :    0   Min.   :0.0000   Min.   : 0.000   Min.   :    0  
    ##  1st Qu.:    0   1st Qu.:1.0000   1st Qu.:11.000   1st Qu.:    0  
    ##  Median :    0   Median :1.0000   Median :11.000   Median :    0  
    ##  Mean   :  277   Mean   :0.8621   Mean   : 9.412   Mean   : 1255  
    ##  3rd Qu.:    0   3rd Qu.:1.0000   3rd Qu.:11.000   3rd Qu.: 3100  
    ##  Max.   :70100   Max.   :2.0000   Max.   :20.000   Max.   :59300  
    ##                                                                   
    ##     TRANWORK         TRANTIME         DEPARTS           in_NYC      
    ##  Min.   : 0.000   Min.   :  0.00   Min.   :   0.0   Min.   :0.0000  
    ##  1st Qu.: 0.000   1st Qu.:  0.00   1st Qu.:   0.0   1st Qu.:0.0000  
    ##  Median : 0.000   Median :  0.00   Median :   0.0   Median :0.0000  
    ##  Mean   : 9.725   Mean   : 14.75   Mean   : 373.3   Mean   :0.3615  
    ##  3rd Qu.:10.000   3rd Qu.: 20.00   3rd Qu.: 732.0   3rd Qu.:1.0000  
    ##  Max.   :70.000   Max.   :138.00   Max.   :2345.0   Max.   :1.0000  
    ##                                                                     
    ##     in_Bronx       in_Manhattan       in_StatenI       in_Brooklyn   
    ##  Min.   :0.0000   Min.   :0.00000   Min.   :0.00000   Min.   :0.000  
    ##  1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.000  
    ##  Median :0.0000   Median :0.00000   Median :0.00000   Median :0.000  
    ##  Mean   :0.0538   Mean   :0.04981   Mean   :0.02084   Mean   :0.126  
    ##  3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.000  
    ##  Max.   :1.0000   Max.   :1.00000   Max.   :1.00000   Max.   :1.000  
    ##                                                                      
    ##    in_Queens      in_Westchester      in_Nassau          Hispanic     
    ##  Min.   :0.0000   Min.   :0.00000   Min.   :0.00000   Min.   :0.0000  
    ##  1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.0000  
    ##  Median :0.0000   Median :0.00000   Median :0.00000   Median :0.0000  
    ##  Mean   :0.1111   Mean   :0.04413   Mean   :0.07032   Mean   :0.1387  
    ##  3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.0000  
    ##  Max.   :1.0000   Max.   :1.00000   Max.   :1.00000   Max.   :1.0000  
    ##                                                                       
    ##     Hisp_Mex          Hisp_PR         Hisp_Cuban         Hisp_DomR      
    ##  Min.   :0.00000   Min.   :0.0000   Min.   :0.000000   Min.   :0.00000  
    ##  1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.000000   1st Qu.:0.00000  
    ##  Median :0.00000   Median :0.0000   Median :0.000000   Median :0.00000  
    ##  Mean   :0.01626   Mean   :0.0436   Mean   :0.003403   Mean   :0.02827  
    ##  3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.000000   3rd Qu.:0.00000  
    ##  Max.   :1.00000   Max.   :1.0000   Max.   :1.000000   Max.   :1.00000  
    ##                                                                         
    ##      white             AfAm          Amindian            Asian        
    ##  Min.   :0.0000   Min.   :0.000   Min.   :0.000000   Min.   :0.00000  
    ##  1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.000000   1st Qu.:0.00000  
    ##  Median :1.0000   Median :0.000   Median :0.000000   Median :0.00000  
    ##  Mean   :0.6997   Mean   :0.125   Mean   :0.003779   Mean   :0.08656  
    ##  3rd Qu.:1.0000   3rd Qu.:0.000   3rd Qu.:0.000000   3rd Qu.:0.00000  
    ##  Max.   :1.0000   Max.   :1.000   Max.   :1.000000   Max.   :1.00000  
    ##                                                                       
    ##     race_oth        unmarried       veteran        has_AnyHealthIns
    ##  Min.   :0.0000   Min.   :0.00   Min.   :0.00000   Min.   :0.0000  
    ##  1st Qu.:0.0000   1st Qu.:0.00   1st Qu.:0.00000   1st Qu.:1.0000  
    ##  Median :0.0000   Median :0.00   Median :0.00000   Median :1.0000  
    ##  Mean   :0.1324   Mean   :0.45   Mean   :0.04443   Mean   :0.9513  
    ##  3rd Qu.:0.0000   3rd Qu.:1.00   3rd Qu.:0.00000   3rd Qu.:1.0000  
    ##  Max.   :1.0000   Max.   :1.00   Max.   :1.00000   Max.   :1.0000  
    ##                                                                    
    ##  has_PvtHealthIns  Commute_car      Commute_bus      Commute_subway   
    ##  Min.   :0.0000   Min.   :0.0000   Min.   :0.00000   Min.   :0.00000  
    ##  1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000  
    ##  Median :1.0000   Median :0.0000   Median :0.00000   Median :0.00000  
    ##  Mean   :0.6906   Mean   :0.2997   Mean   :0.02162   Mean   :0.07468  
    ##  3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:0.00000  
    ##  Max.   :1.0000   Max.   :1.0000   Max.   :1.00000   Max.   :1.00000  
    ##                                                                       
    ##   Commute_rail     Commute_other     below_povertyline below_150poverty
    ##  Min.   :0.00000   Min.   :0.00000   Min.   :0.000     Min.   :0.0000  
    ##  1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.000     1st Qu.:0.0000  
    ##  Median :0.00000   Median :0.00000   Median :0.000     Median :0.0000  
    ##  Mean   :0.01332   Mean   :0.05506   Mean   :0.122     Mean   :0.1965  
    ##  3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.000     3rd Qu.:0.0000  
    ##  Max.   :1.00000   Max.   :1.00000   Max.   :1.000     Max.   :1.0000  
    ##                                                                        
    ##  below_200poverty   foodstamps    
    ##  Min.   :0.0000   Min.   :0.0000  
    ##  1st Qu.:0.0000   1st Qu.:0.0000  
    ##  Median :0.0000   Median :0.0000  
    ##  Mean   :0.2676   Mean   :0.1465  
    ##  3rd Qu.:1.0000   3rd Qu.:0.0000  
    ##  Max.   :1.0000   Max.   :1.0000  
    ## 

``` r
print(NN_obs <- length(AGE))
```

    ## [1] 196585

So this shows that there are 196,585 people in this dataset.

\#\#\#Simple Stats We compare the average age of the men and the women
in the data,

``` r
summary(AGE[female == 1])
```

    ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    ##    0.00   23.00   44.00   42.72   61.00   95.00

``` r
summary(AGE[!female])
```

    ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    ##    0.00   21.00   40.00   40.35   59.00   95.00

This uses the female dummy variable. The comparison is between those who
have the variable female=1 (i.e. women) and those not female=1 (so
logical not, denoted with the “\!” symbol, i.e. men). *I know, you can –
and people do\! – worry that this binary classification for gender
misses some people; government statistics are just not there yet.
Progress is slower than we’d like.*

Women in this dataset are, on average, a bit older, with an average age
of 42.7 compared with 40.4 for men. You might wonder (if you were to
begin to think like a statistician) whether that is a big difference -
hold onto that thought\! Alternately you can use the commands to
calculate the average, mean(), and the standard deviation, sd(), to get
those statistics:

``` r
# here i want to find average ages of men and women
mean(AGE[female == 1])
```

    ## [1] 42.71629

``` r
sd(AGE[female == 1])
```

    ## [1] 23.72012

``` r
mean(AGE[!female])
```

    ## [1] 40.35398

``` r
sd(AGE[!female])
```

    ## [1] 23.1098

Later you might encounter cases where you want more complicated dummy
variables and want to use logical relations “and” “or” “not” (the
symbols “&”, “|”, “\!”) or the “\>=” or multiplication or division.

As you’re going along, if you’re copying-and-pasting then you might not
have had trouble, but if you’re typing then you’ve probably realized
that R is persnickety - the variable AGE is not equivalent to Age nor
age nor aGe… You might be wondering if there’s an easier way than
copy-paste; *there is*. If you highlight text in the Source pane and
then hit CTRL-Enter, that will run the block of code. Alt if you click
the “Knit HTML” button from the Rmd file then that will recreate the
webpage as well as run all of the commands.

\#\#\#More Details At the beginning you’ll be just copying commands that
I give you but as you start making changes, you have to understand
what’s going on.

For better or worse there is rarely just a single way of doing things,
when people talk about the R language it does indeed have many features
of a language. Just like a spoken language has many ways of saying “hi”
so too there are lots of different ways to ask R to produce means of
different groups. I am showing you a few particular ways but if you look
around online you’ll find others.

Remember to save your work. The computers in the lab wipe the memory
clean when you log off so back up your files. GitHub is the best way to
do that – especially since you’ll be collaborating on work.

But be careful - you have the option of saving your code and/or your
workspace; it is always better to save just the code. For example, your
code might load some data (at this stage, usually data that I’ve
provided). If you make changes to this data during your session, you do
not want to restart with those changes. If you just save your code and
re-run the program, you will start fresh. If instead you save the
workspace, then you end up saving all of your scraps and changes - which
eventually start to build up. So only save your workspace if you’ve run
a large chunk of code that you don’t want to have to later re-run. (This
is even more true if you “attach” data frames without later “detach”-ing
them\!)

This is a whole different approach to workflow and project management.
The old bad way is to have something like an Excel sheet, where a human
goes through and makes a few changes to the original data – and these
changes might well be smart and good\! But they don’t replicate and
they’re even tough to remember after some time. And it’s tough to
track back and find errors, whether they are typos or were in the
original data.

There are 2 main file types: R script (lines of instructions to R on how
to calculate some statistics) and R Markdown (which combines the
instructions to R along with text around it, which is how I created this
file). There are more types of course but those are the ones for now.

There is a bigger philosophy behind this: the idea of “reproducible
research,” that if you share your .Rmd file then anybody else can run
the exact same program and get the exact same results. It’s a way of
convincing an audience that there was no skulduggery in how the data
relates to the conclusion. It’s useful in class since it’s an easy way
to submit homework or share work with your study group.

Sharing is caring. You’ve got to get in the habit of knowing that your
code is not some personal scratch file that only has to work for you.
Most workplaces are in teams so the code has to be understandable more
broadly. And if you recall from Behavioral Economics, there are times
where it’s useful to think of your future self as a different person. (I
do that\! I load old code that I wrote and sometimes yell, “I hate the
frickin a-hole who wrote this code\!”) So set good habits right from the
beginning. If you have your own desk in your own office then maybe it’s
a mess, that’s your choice. But if you share a space then you know you
have to be neater and not be that slob that everybody else hates. Your
coding too. You will share code with your study group.

You will put your code on GitHub – that’s how you will work with your
study group and how you’ll submit homework. If you decide to work in the
field, you will want to share examples of your code and GitHub is the
standard.

Create an account on GitHub – first [here’s Jenny Bryan with a great
explanation of the
why](https://dx.doi.org/10.7287%2Fpeerj.preprints.3159v2), then [she
gives complete instructions here](https://happygitwithr.com/index.html).
Go read the first about why, then read her parts 4 and 5 and create your
account. You’ll eventually read the whole thing carefully.

Each set of coding should be a Project – in RStudio, that’s a particular
thing. In File menu, find “New Project”. Each project has its own folder
along with subfolders within for data and output. Above I asked you to
create a R folder with subfolders within for each project including
ecob2000\_lecture1 for this one.

In your code, use the comments generously. I sometimes first use the
comments to write what I’m trying to do as a way to set down my goals
for each segment. Then can implement them with the following code.
Again, be mindful that other people (and your future self) will be
trying to follow along with your code to understand what’s going on.

In the next part (Lecture 1 A) I’ll go a bit deeper into the coding,
talk about packages in R, and get into some details about the PUMS data.
